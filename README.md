# GPT-Style Prompt Continuation

A deep learning project exploring transfer learning, fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) techniques for text generation using GPT-based models.

## ðŸ“‹ Overview

This project demonstrates advanced natural language processing techniques by fine-tuning pretrained transformer models on specialized documents (UNDRIP and Economic Reports). It compares traditional full fine-tuning with modern PEFT approaches, specifically LoRA (Low-Rank Adaptation).

## ðŸŽ¯ Key Features

- **Transfer Learning**: Leverages pretrained GPT-2 and GPT-Neo models
- **Document Processing**: Extracts and cleans text from PDF documents
- **Multiple Fine-Tuning Approaches**:
  - Full fine-tuning on domain-specific data
  - LoRA-based Parameter-Efficient Fine-Tuning
- **Comprehensive Evaluation**: Uses perplexity metrics and prompt continuation
- **Model Comparison**: Benchmarks baseline vs fine-tuned vs PEFT models

## ðŸ—ï¸ Architecture

### Models Explored

1. **GPT-2** (124M parameters)
   - Base model for initial experiments
   - 12 transformer blocks with self-attention

2. **GPT-Neo-125M** (125M parameters)
   - Variant model showing improved generalization
   - Enhanced performance on unseen data

3. **LoRA PEFT Model**
   - Only 294,912 trainable parameters (0.24% of total)
   - 99.76% reduction in trainable parameters
   - Comparable performance to full fine-tuning

## ðŸ“Š Results Summary

| Model | UNDRIP PPL | Economic PPL | Trainable Params | Training Time |
|-------|------------|--------------|------------------|---------------|
| GPT-2 (Baseline) | 19.37 | 20.53 | 163M | N/A |
| GPT-2 (Fine-tuned) | 16.88 | 18.02 | 163M | ~800s |
| GPT-Neo (Baseline) | 19.37 | 20.53 | 125M | N/A |
| GPT-Neo (Fine-tuned) | 15.68 | 17.25 | 125M | ~2400s |
| LoRA (Fine-tuned) | 16.45 | 17.89 | 294K | ~1800s |

*Lower perplexity indicates better model performance*

## ðŸš€ Getting Started

### Prerequisites

```bash
pip install torch transformers peft accelerate pdfplumber torchinfo
```

### Installation

```bash
git clone https://github.com/VK4041/GPT-Style_Prompt_Continuation.git
cd GPT-Style_Prompt_Continuation
```

### Running the Notebook

1. Mount Google Drive (if using Colab)
2. Place your PDF documents in the appropriate directory
3. Run cells sequentially
4. Models will be saved to `./fine_tuned_gpt2`, `./fine_tuned_variant`, and `./fine_tuned_lora_peft`

## ðŸ“– Project Structure

```
â”œâ”€â”€ Data Extraction & Cleaning
â”‚   â””â”€â”€ PDF text extraction with preprocessing
â”œâ”€â”€ Baseline Model Evaluation
â”‚   â””â”€â”€ Perplexity computation on pretrained models
â”œâ”€â”€ Full Fine-Tuning
â”‚   â”œâ”€â”€ GPT-2 fine-tuning
â”‚   â””â”€â”€ GPT-Neo fine-tuning
â”œâ”€â”€ PEFT with LoRA
â”‚   â”œâ”€â”€ LoRA configuration
â”‚   â”œâ”€â”€ Efficient fine-tuning
â”‚   â””â”€â”€ Performance comparison
â””â”€â”€ Evaluation & Analysis
    â”œâ”€â”€ Perplexity metrics
    â””â”€â”€ Qualitative text generation
```

## ðŸ”§ Training Configuration

### Hyperparameters

- **Learning Rate**: 1e-5 (full fine-tuning), 3e-5 (LoRA)
- **Epochs**: 3-5 with early stopping
- **Batch Size**: 2 (due to GPU constraints)
- **Optimizer**: AdamW with weight decay (0.01)
- **Scheduler**: Linear warmup (10% of steps)
- **Context Length**: 1024 tokens
- **Stride**: 512 tokens (for sliding window)

### Early Stopping

- **Patience**: 3 epochs
- **Min Delta**: 0.1 (full fine-tuning), 0.01 (LoRA)

## ðŸ’¡ Key Techniques

### Data Processing

- Page-wise PDF text extraction using `pdfplumber`
- Whitespace normalization and header/footer removal
- Non-ASCII character filtering
- Sliding window chunking for long documents

### LoRA Configuration

```python
LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=16,
    lora_dropout=0.2,
    bias="none"
)
```

### Perplexity Evaluation

Uses sliding window approach with overlapping chunks to compute model uncertainty on text sequences.

## ðŸ“ˆ Insights

### Pretrained Model
- General, bureaucratic tone
- Lacks domain-specific depth
- Broader generalization

### LoRA Fine-Tuned Model
- Data-driven, analytical approach
- Provides statistics with moderate interpretation
- **99.76% fewer parameters** than full fine-tuning
- Achieves near-equivalent performance

### Fully Fine-Tuned Model
- Strong domain adaptation
- Human-rights advocacy tone
- Best performance on unseen data
- Risk of slight overfitting

## ðŸŽ“ Academic Context

**Course**: SIT744 Deep Learning - Deakin University  
**Author**: Varun Kumar

This project demonstrates practical applications of:
- Transfer learning in NLP
- Domain adaptation techniques
- Parameter-efficient fine-tuning methods
- Model evaluation and comparison

## ðŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{kumar2024gpt_prompt_continuation,
  author = {Kumar, Varun},
  title = {GPT-Style Prompt Continuation with PEFT},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/VK4041/GPT-Style_Prompt_Continuation}
}
```

## âš ï¸ Notes

- Designed for Google Colab with GPU support >= L4 High RAM GPU
- Requires significant computational resources
- PDF files included
- CUDA memory optimizations included for T4 GPU
- Trained on A100 GPU

## ðŸ“„ License

This project is open source and available under the MIT License.

## ðŸ”— Links

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [PEFT Library](https://huggingface.co/docs/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

## ðŸ“§ Contact

Email me: varunvk4041@gmail.com

Find the resources here: https://drive.google.com/drive/folders/14TOWegOtAg8Tfdjy9NRyVN0jLoLlrlA7?usp=sharing

---

**Note**: This project was developed as part of academic coursework. The techniques demonstrated are applicable to various NLP tasks requiring domain adaptation with limited computational resources.
